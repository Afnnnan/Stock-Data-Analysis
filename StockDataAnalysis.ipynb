{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E9-2\n",
    "#### This Notebook illustrates the use of SPARK Dataframe functions to process nsedata.csv\n",
    "- Review <b>Part-1</b> to understand the code by referring to SPARK documentation.\n",
    "- Add your comment to each cell, to explain its purpose\n",
    "- Add code / create additional cells for debugging purpose, and comment them too \n",
    "- Write SPARK code to solve the problem stated in <b>Part-2</b> (do not use the createTempView function in your solution!)\n",
    "\n",
    "<b>Submission</b>\n",
    "- Create and upload a PDF of this Notebook.\n",
    "- <b> BEFORE CONVERTING TO PDF ENSURE THAT YOU REMOVE / TRIM LENGTHY DEBUG OUTPUTS </b>.\n",
    "- Short debug outputs of up to 5 lines are acceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Part 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark  # Importing the findspark library to locate the Spark installation\n",
    "findspark.init()  # Initializing findspark to set up the Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark  # Importing the pyspark library for Spark functionality\n",
    "from pyspark.sql.types import *  # Importing data types from pyspark.sql.types module\n",
    "from pyspark.sql import functions as F  # Importing Spark SQL functions and aliasing it as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"E9-2\")  # Creating a SparkContext with the specified application name \"E9-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pyspark.sql.SparkSession(sc)  # Creating a SparkSession using the existing SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = ss.read  # Creating a DataFrameReader object to read data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('SYMBOL', StringType(), True), StructField('SERIES', StringType(), True), StructField('OPEN', DoubleType(), True), StructField('HIGH', DoubleType(), True), StructField('LOW', DoubleType(), True), StructField('CLOSE', DoubleType(), True), StructField('LAST', DoubleType(), True), StructField('PREVCLOSE', DoubleType(), True), StructField('TOTTRDQTY', LongType(), True), StructField('TOTTRDVAL', DoubleType(), True), StructField('TIMESTAMP', StringType(), True), StructField('ADDNL', StringType(), True)])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the schemaStruct using StructType to specify the structure of the DataFrame\n",
    "schemaStruct = StructType()  # Creating an empty StructType object\n",
    "\n",
    "# Adding fields to the schemaStruct with their name, data type, and nullable flag\n",
    "schemaStruct.add(\"SYMBOL\", StringType(), True)\n",
    "schemaStruct.add(\"SERIES\", StringType(), True)\n",
    "schemaStruct.add(\"OPEN\", DoubleType(), True)\n",
    "schemaStruct.add(\"HIGH\", DoubleType(), True)\n",
    "schemaStruct.add(\"LOW\", DoubleType(), True)\n",
    "schemaStruct.add(\"CLOSE\", DoubleType(), True)\n",
    "schemaStruct.add(\"LAST\", DoubleType(), True)\n",
    "schemaStruct.add(\"PREVCLOSE\", DoubleType(), True)\n",
    "schemaStruct.add(\"TOTTRDQTY\", LongType(), True)\n",
    "schemaStruct.add(\"TOTTRDVAL\", DoubleType(), True)\n",
    "schemaStruct.add(\"TIMESTAMP\", StringType(), True)\n",
    "schemaStruct.add(\"ADDNL\", StringType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfr.csv(\"/home/hduser/spark/nsedata.csv\", schema=schemaStruct, header=True)\n",
    "# Reading the CSV file located at \"/home/hduser/spark/nsedata.csv\" into a DataFrame(df) with the specified schemaStruct and header=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Basics : Using SPARK for analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_from_df(company_code):\n",
    "    \"\"\"\n",
    "    Function to create a subset DataFrame for a specific company code.\n",
    "    \n",
    "    Args:\n",
    "        company_code (str): The company code for which the subset DataFrame is created.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Subset DataFrame containing columns for the specified company code.\n",
    "    \"\"\"\n",
    "    tcode = company_code.lower()  # Converting company_code to lowercase\n",
    "    # Selecting specific columns and renaming them with the company code\n",
    "    df_subset = df.select(\\\n",
    "                    F.col(\"OPEN\").alias(\"OPEN_\" + tcode),\\\n",
    "                    F.col(\"HIGH\").alias(\"HIGH_\"+ tcode),\\\n",
    "                    F.col(\"LOW\").alias(\"LOW_\"+ tcode),\\\n",
    "                    F.col(\"CLOSE\").alias(\"CLOSE_\" + tcode),\\\n",
    "                    F.col(\"TIMESTAMP\")).\\\n",
    "                    where(F.col(\"SYMBOL\") == company_code)  # Filtering rows for the specified company code\n",
    "    return(df_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we need to use the alias function, above? What happens if we do not alias / rename the columns?\n",
    "\n",
    "# Using alias allows dynamic renaming of columns. If we don't alias, columns from different DataFrames may have conflicting names,\n",
    "# causing ambiguity and potential data loss during operations like joining or merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+----------+-----------+\n",
      "|OPEN_infy|HIGH_infy|LOW_infy|CLOSE_infy|  TIMESTAMP|\n",
      "+---------+---------+--------+----------+-----------+\n",
      "|  2910.75|  2953.55| 2910.75|    2944.2|01-APR-2013|\n",
      "|  3283.05|   3325.0|  3275.0|   3313.95|01-APR-2014|\n",
      "|   2198.9|   2199.5|  2157.7|   2173.95|01-APR-2015|\n",
      "|   2780.1|   2823.8|  2780.1|    2815.1|01-AUG-2011|\n",
      "|   2215.0|   2230.4| 2200.75|   2218.55|01-AUG-2012|\n",
      "+---------+---------+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 00:59:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+-----------+\n",
      "|summary|         OPEN_infy|        HIGH_infy|          LOW_infy|        CLOSE_infy|  TIMESTAMP|\n",
      "+-------+------------------+-----------------+------------------+------------------+-----------+\n",
      "|  count|              1023|             1023|              1023|              1023|       1023|\n",
      "|   mean|2707.2617302052795|2735.799413489735|2679.3007331378312|2708.2897849462406|       NULL|\n",
      "| stddev| 641.7956780031782|647.6486140678146| 639.1258920145511| 643.8471963450963|       NULL|\n",
      "|    min|             941.0|            952.1|            932.65|             937.5|01-APR-2013|\n",
      "|    max|            4387.0|           4402.2|            4343.4|            4365.9|31-OCT-2014|\n",
      "+-------+------------------+-----------------+------------------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating a subset DataFrame for company code \"INFY\"\n",
    "df_infy = create_subset_from_df(\"INFY\")\n",
    "# Showing the first 5 rows of the subset DataFrame\n",
    "df_infy.show(5)\n",
    "# Generating summary statistics for the subset DataFrame\n",
    "df_infy.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+---------+-----------+\n",
      "|OPEN_tcs|HIGH_tcs|LOW_tcs|CLOSE_tcs|  TIMESTAMP|\n",
      "+--------+--------+-------+---------+-----------+\n",
      "|  1185.0| 1198.75|1172.55|  1180.15|01-APR-2011|\n",
      "|  1565.0|  1573.7|1551.25|  1556.85|01-APR-2013|\n",
      "|  2145.0|  2185.0| 2144.9|   2176.7|01-APR-2014|\n",
      "|  2558.0|  2563.6|2522.25|  2542.65|01-APR-2015|\n",
      "|  1142.4|  1149.9| 1125.1|  1135.25|01-AUG-2011|\n",
      "+--------+--------+-------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+-----------+\n",
      "|summary|          OPEN_tcs|          HIGH_tcs|           LOW_tcs|        CLOSE_tcs|  TIMESTAMP|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------+\n",
      "|  count|              1240|              1240|              1240|             1240|       1240|\n",
      "|   mean|1678.2334677419353|1697.3971370967743|1658.5663306451613|1677.914798387098|       NULL|\n",
      "| stddev| 598.9262836758081| 603.7783546396355| 592.8736960983055|597.8815093696456|       NULL|\n",
      "|    min|             838.0|             847.7|             830.2|            837.3|01-APR-2011|\n",
      "|    max|            2788.0|            2839.7|            2737.0|           2776.0|31-OCT-2014|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating a subset DataFrame for company code \"TCS\"\n",
    "df_tcs = create_subset_from_df(\"TCS\")\n",
    "# Showing the first 5 rows of the subset DataFrame\n",
    "df_tcs.show(5)\n",
    "# Generating summary statistics for the subset DataFrame\n",
    "df_tcs.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                  (0 + 2) / 2][Stage 11:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|  TIMESTAMP|CLOSE_tcs|CLOSE_infy|\n",
      "+-----------+---------+----------+\n",
      "|02-FEB-2012|   1148.0|    2757.0|\n",
      "|04-DEC-2014|  2637.95|    2101.8|\n",
      "|01-AUG-2013|   1815.4|   2974.65|\n",
      "|01-DEC-2014|  2692.95|   4349.85|\n",
      "|08-FEB-2012|  1219.65|   2769.15|\n",
      "+-----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Joining the subset DataFrames for TCS and INFY on the \"TIMESTAMP\" column\n",
    "df_join = df_tcs.join(df_infy, \"TIMESTAMP\").select(\"TIMESTAMP\", \"CLOSE_tcs\", \"CLOSE_infy\")\n",
    "# Selecting specific columns (\"TIMESTAMP\", \"CLOSE_tcs\", \"CLOSE_infy\") from the joined DataFrame\n",
    "df_join.show(5)  # Showing the first 5 rows of the joined DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         PriceDiff|\n",
      "+-------+------------------+\n",
      "|  count|              1025|\n",
      "|   mean|1163.6446341463443|\n",
      "| stddev|  366.989701532277|\n",
      "|    min|150.95000000000027|\n",
      "|    max|            1804.9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the absolute difference between the \"CLOSE_tcs\" and \"CLOSE_infy\" columns and aliasing it as \"PriceDiff\"\n",
    "price_diff = F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]).alias(\"PriceDiff\")\n",
    "# Selecting the \"PriceDiff\" column and generating summary statistics for it\n",
    "df_join.select(price_diff).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|  TIMESTAMP|CLOSE_tcs|CLOSE_infy|\n",
      "+-----------+---------+----------+\n",
      "|12-FEB-2015|  2462.15|    2311.2|\n",
      "|11-FEB-2015|   2459.9|   2284.85|\n",
      "|10-FEB-2015|  2441.15|    2278.3|\n",
      "+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering the joined DataFrame to include rows where the absolute difference between \"CLOSE_tcs\" and \"CLOSE_infy\" is less than 180\n",
    "filtered_df = df_join.filter(F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]) < 180)\n",
    "# Showing the filtered DataFrame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, to_date  # Importing necessary functions\n",
    "\n",
    "# Adding a new column \"TIMESTAMP2\" to the DataFrame(df) by converting \"TIMESTAMP\" to date format and then formatting it to \"yyyy-MM\"\n",
    "df1 = df.withColumn(\"TIMESTAMP2\", date_format(to_date(col(\"TIMESTAMP\"), \"dd-MMM-yyyy\"), \"yyyy-MM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SYMBOL: string (nullable = true)\n",
      " |-- SERIES: string (nullable = true)\n",
      " |-- OPEN: double (nullable = true)\n",
      " |-- HIGH: double (nullable = true)\n",
      " |-- LOW: double (nullable = true)\n",
      " |-- CLOSE: double (nullable = true)\n",
      " |-- LAST: double (nullable = true)\n",
      " |-- PREVCLOSE: double (nullable = true)\n",
      " |-- TOTTRDQTY: long (nullable = true)\n",
      " |-- TOTTRDVAL: double (nullable = true)\n",
      " |-- TIMESTAMP: string (nullable = true)\n",
      " |-- ADDNL: string (nullable = true)\n",
      " |-- TIMESTAMP2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema of the DataFrame df1 to display its structure\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "|    SYMBOL|SERIES|  OPEN|   HIGH|   LOW| CLOSE|  LAST|PREVCLOSE|TOTTRDQTY|    TOTTRDVAL|  TIMESTAMP|ADDNL|TIMESTAMP2|\n",
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "| 20MICRONS|    EQ| 37.75|  37.75| 36.35| 37.45|  37.3|    37.15|    38638|    1420968.1|01-APR-2011|    0|   2011-04|\n",
      "|3IINFOTECH|    EQ| 43.75|   45.3| 43.75|  44.9|  44.8|    43.85|  1239690|5.531120435E7|01-APR-2011|    0|   2011-04|\n",
      "|   3MINDIA|    EQ|3374.0|3439.95|3338.0|3397.5|3400.0|   3364.7|      871|   2941547.35|01-APR-2011|    0|   2011-04|\n",
      "|    A2ZMES|    EQ| 281.8| 294.45| 279.8| 289.2| 287.2|    281.3|   140643| 4.02640755E7|01-APR-2011|    0|   2011-04|\n",
      "|AARTIDRUGS|    EQ| 127.0|  132.0|126.55| 131.3| 130.6|    127.6|     2972|     384468.2|01-APR-2011|    0|   2011-04|\n",
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 00:59:31 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 12\n",
      "CSV file: file:///home/hduser/spark/nsedata.csv\n"
     ]
    }
   ],
   "source": [
    "# Showing the first 5 rows of the DataFrame df1\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F  # Importing necessary functions\n",
    "\n",
    "# Grouping the DataFrame df1 by \"SYMBOL\" and \"TIMESTAMP2\", and aggregating various statistics for the \"OPEN\" column\n",
    "df_t1 = df1.groupBy(\"SYMBOL\", \"TIMESTAMP2\").agg(\n",
    "    F.min(\"OPEN\"), F.max(\"OPEN\"), F.avg(\"OPEN\"),  # Calculating min, max, and average values\n",
    "    F.stddev(\"OPEN\"),  # Calculating standard deviation\n",
    "    F.count(\"OPEN\")  # Counting the number of values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|    SYMBOL|TIMESTAMP2|min(OPEN)|max(OPEN)|         avg(OPEN)|      stddev(OPEN)|count(OPEN)|\n",
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|  AREVAT&D|   2011-04|   246.15|   292.95|274.73055555555555|12.609988324957133|         18|\n",
      "| CHEMPLAST|   2011-04|      6.3|     8.25| 7.172222222222222|0.5570991627387202|         18|\n",
      "|FIRSTLEASE|   2011-04|     68.3|   106.05| 93.05277777777778|10.687822540330412|         18|\n",
      "|    FORTIS|   2011-04|    152.0|    163.4|159.50833333333333|2.7349723087102324|         18|\n",
      "| GOLDINFRA|   2011-04|    16.85|    20.15|17.924999999999997|0.7857648952379039|         18|\n",
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Showing the first 5 rows of the DataFrame df_t1\n",
    "df_t1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the DataFrame df_t1 in ascending order based on \"SYMBOL\" and \"TIMESTAMP2\" columns\n",
    "df_t2 = df_t1.sort(F.asc(\"SYMBOL\"), F.asc(\"TIMESTAMP2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|   SYMBOL|TIMESTAMP2|min(OPEN)|max(OPEN)|         avg(OPEN)|      stddev(OPEN)|count(OPEN)|\n",
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|20MICRONS|   2010-08|     51.6|     54.0| 52.81666666666667|0.9266876496425305|          9|\n",
      "|20MICRONS|   2010-09|     54.9|     64.3| 59.11428571428571| 2.514614426564382|         21|\n",
      "|20MICRONS|   2010-10|    55.05|     60.0|57.166666666666664|1.3035848009751156|         21|\n",
      "|20MICRONS|   2010-11|     53.6|    61.75| 55.98809523809524|2.2001650370997603|         21|\n",
      "|20MICRONS|   2010-12|     38.8|     61.0| 45.66590909090909| 5.796599708606606|         22|\n",
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Showing the first 5 rows of the sorted DataFrame df_t2\n",
    "df_t2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Uncomment the following statement to generate the output, and analyze it\n",
    "# Write your observations in the next cell\n",
    "\n",
    "# Writing the DataFrame df_t2 to a CSV file named \"monthly_stats.csv\"\n",
    "df_t2.write.csv(\"monthly_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Observations of monthly_stats.csv</b>\n",
    "\n",
    "- The CSV file contains monthly statistics for different symbols.\n",
    "- Each row represents a combination of symbol and timestamp, indicating the statistics for that symbol in a particular month.\n",
    "- The statistics include minimum, maximum, average, and standard deviation of the opening prices, along with the count of opening prices.\n",
    "- The data is sorted in ascending order based on symbol and timestamp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b>SPARK based solutions for stock analysis and portfolio management: An Example</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Based on equity (EQ) data contained in nsedata.csv, you are tasked with the responsibility to identify a set of 10 stocks to invest in based on the following steps:\n",
    "\n",
    "- You have to process the data for one entire year, and then make investment decisions for the following year. You can shoose 2012 as the past year and make recommendations for 2013.\n",
    "- Assume that you are doing this analysis on Jan 1, 2013. \n",
    "- You are required to draw up an initial list of 10 stocks based on the following preliminary analysis: \n",
    "    - The stocks should be liquid. That is, they should be traded in large volumes almost every day and the trading volume should be high.\n",
    "    - You have to filter those stocks that have shown maximum overall growth over the past year. The hope is that they will continue to grow in the future.\n",
    "- Select 5 pairs of stocks from these filtered stocks based on the following further analysis.\n",
    "    - You should ensure that volatility and negative market movements in the coming year will not adversely affect the total investment, substantially.\n",
    "    - One way to achieve this involves selecting <b>stock pairs that are negatively correlated</b>, so that if one stock loses value its partner will most likely gain value - thereby reducing the overall impact of fall in stock prices. As all these stocks are high growth stocks, anyway, the expectation is that there also will be overall growth of the portfolio. \n",
    "    - Purchase 1 unit of each of these stock pairs on the first trading day of the next year (i.e. 2013)\n",
    "- Once you have selected the 5 pairs and made the above investments, you should further do the following\n",
    "    - Report the performance of your portfolio as on 31/12/2013 (or the nearest traded date, if 31/12/2013 was a non traded day) in terms of the:\n",
    "        - Overall growth of your portfolio\n",
    "        - Report which stocks in your portfolio grew in value, which of them reduced in value, an whether the pairing strategy worked.\n",
    "        - How did the overall market perform during the same period? This can be assessed as follows:\n",
    "            - If you had blindly selected 1 stock each of the top 25 highly traded, high growth stocks, what would have been the performance of this portfolio\n",
    "            - How did the implemented strategy of selecting highly traded, high growth stocks, but in pairs having <b>negative correlation</b>, perform in comparion? Did the strategy work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some suggested steps to solve the problem\n",
    "\n",
    "# First of all select only EQUITY related data\n",
    "# Create a dataframe of stocks that have traded in during the year 2012\n",
    "# Find out the average total traded quantity of each of these stocks\n",
    "# Identify stocks that high trade volumes: average daily volume ranging between 5L and 10L\n",
    "# Find out the price difference in each of these stocks between the 'last traded day of 2012' and 'first traded day of 2012'\n",
    "# Sort the stocks in descending order using traded quantity and price difference as the criteria\n",
    "# Select the top 10 stocks for further analysis\n",
    "# Create a new dataframe containing pairs of stocks traded on the same day \n",
    "#   - join the selected stocks by using the criteria that stock names in the resulting dataframe are different\n",
    "# Sort the dataframe in ascending order\n",
    "# Establish the criteria for selecting the final pairs of stocks, and select them\n",
    "# Calculate your total investment value\n",
    "# ... likewise state and complete the rest of the steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the DataFrame to select only EQUITY related data for the year 2012\n",
    "df_2012 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2012'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  SYMBOL|   avg(TOTTRDQTY)|\n",
      "+--------+-----------------+\n",
      "|GMRINFRA|8600963.744939271|\n",
      "|HINDALCO|8189032.975708502|\n",
      "|  RENUKA|7831300.113360324|\n",
      "|    STER|7603208.680161944|\n",
      "|    IDFC|7106657.668016194|\n",
      "|     DLF|6902130.275303644|\n",
      "|ASHOKLEY|6823850.761133603|\n",
      "|     ITC|6474399.400809716|\n",
      "|ALOKTEXT|6395331.052631579|\n",
      "|    NHPC|6060316.226720648|\n",
      "+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating the average total traded quantity for each symbol in the DataFrame df_2012\n",
    "# Filtering symbols with average total traded quantity between 5L and 10L\n",
    "# Sorting the DataFrame in descending order based on average total traded quantity\n",
    "df_2012_avgqty = df_2012.groupBy(\"SYMBOL\").avg(\"TOTTRDQTY\")\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") < 10000000)\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") > 500000)\\\n",
    "                        .orderBy(\"avg(TOTTRDQTY)\", ascending=False)\n",
    "# Showing the top 10 symbols with high average total traded quantity\n",
    "df_2012_avgqty.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top 10 symbols with high average total traded quantity\n",
    "top10 = df_2012_avgqty.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting the symbols from df_2012_avgqty DataFrame\n",
    "t1 = top10.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filtering the DataFrame df_2012 to include only the selected symbols\n",
    "t2 = df_2012.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "\n",
    "# Creating DataFrame t3 with columns SYMBOL as S1, CLOSE as Close1, and TIMESTAMP\n",
    "t3 = t2.select(F.col(\"SYMBOL\").alias(\"S1\"), F.col(\"CLOSE\").alias(\"Close1\"), \"TIMESTAMP\")\n",
    "\n",
    "# Creating DataFrame t4 with columns SYMBOL as S2, CLOSE as Close2, and TIMESTAMP\n",
    "t4 = t2.select(F.col(\"SYMBOL\").alias(\"S2\"), F.col(\"CLOSE\").alias(\"Close2\"), \"TIMESTAMP\")\n",
    "\n",
    "# Joining DataFrames t3 and t4 on TIMESTAMP to create DataFrame df_for_corr\n",
    "df_for_corr = t3.join(t4, \"TIMESTAMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+------+\n",
      "|  TIMESTAMP|      S1|Close1|    S2|Close2|\n",
      "+-----------+--------+------+------+------+\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|  STER| 124.1|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|RENUKA| 38.85|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|  NHPC| 20.85|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|   ITC|199.05|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|  IDFC|131.45|\n",
      "+-----------+--------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Showing the first 5 rows of the DataFrame df_for_corr\n",
    "df_for_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(S1='IDFC', S2='NHPC'),\n",
       " Row(S1='RENUKA', S2='IDFC'),\n",
       " Row(S1='ALOKTEXT', S2='ASHOKLEY'),\n",
       " Row(S1='STER', S2='ITC'),\n",
       " Row(S1='RENUKA', S2='STER'),\n",
       " Row(S1='HINDALCO', S2='IDFC'),\n",
       " Row(S1='ITC', S2='ALOKTEXT'),\n",
       " Row(S1='ALOKTEXT', S2='NHPC'),\n",
       " Row(S1='DLF', S2='ALOKTEXT'),\n",
       " Row(S1='GMRINFRA', S2='ITC')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting distinct pairs of symbols where S1 is not equal to S2\n",
    "# Collecting the first 10 pairs as a list\n",
    "wrklist = df_for_corr.select(\"S1\",\"S2\").filter(\"S1 != S2\").distinct().collect()\n",
    "wrklist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Printing the length of the wrklist, which represents the number of pairs in the list\n",
    "print(len(wrklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 20 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 40 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 60 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 80 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initializing an empty list tcorr to store correlation values for stock pairs\n",
    "tcorr = []\n",
    "# Getting the length of wrklist\n",
    "tlen = len(wrklist)\n",
    "# Iterating through each pair of stocks in wrklist\n",
    "for i in range(tlen):\n",
    "    # Extracting the first and second stock symbols from the pair\n",
    "    s1 = wrklist[i][0]\n",
    "    s2 = wrklist[i][1]\n",
    "    # Calculating the correlation between the closing prices of the two stocks\n",
    "    corr = df_for_corr.filter((F.col('S1') == s1) & (F.col('S2') == s2)).corr(\"Close1\", \"Close2\")\n",
    "    # Appending the stock symbols and correlation value to tcorr list\n",
    "    tcorr.append([s1, s2, corr])\n",
    "    # Printing progress message for every 20 iterations\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processed: {i} of {tlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+\n",
      "| Symbol1| Symbol2|      Corr|\n",
      "+--------+--------+----------+\n",
      "|    IDFC|    NHPC| 0.7452768|\n",
      "|  RENUKA|    IDFC| 0.2969912|\n",
      "|ALOKTEXT|ASHOKLEY|0.47407645|\n",
      "|    STER|     ITC|-0.3065829|\n",
      "|  RENUKA|    STER| 0.6944879|\n",
      "+--------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules and classes\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Defining the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Symbol1\", StringType(), True),\n",
    "    StructField(\"Symbol2\", StringType(), True),\n",
    "    StructField(\"Corr\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Creating an RDD from the list of correlations tcorr\n",
    "rdd = sc.parallelize(tcorr)\n",
    "\n",
    "# Creating a DataFrame df_corr from the RDD\n",
    "df_corr = ss.createDataFrame(rdd.map(lambda x: Row(Symbol1=x[0], Symbol2=x[1], Corr=float(x[2]))), schema)\n",
    "\n",
    "# Showing the first 5 rows of the DataFrame df_corr\n",
    "df_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the DataFrame df_corr to include only negative correlations and removing duplicates based on \"Corr\" column\n",
    "# Sorting the DataFrame in ascending order based on \"Corr\" column\n",
    "df_corr_neg = df_corr.filter(F.col(\"Corr\") <= 0.0).dropDuplicates([\"Corr\"]).orderBy(F.col(\"Corr\").asc())\n",
    "\n",
    "# Counting the number of rows in the DataFrame df_corr_neg\n",
    "df_corr_neg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------+\n",
      "| Symbol1| Symbol2|         Corr|\n",
      "+--------+--------+-------------+\n",
      "|     ITC|ALOKTEXT|  -0.90314275|\n",
      "|GMRINFRA|     ITC|   -0.7135044|\n",
      "|    IDFC|ALOKTEXT|   -0.6409445|\n",
      "|HINDALCO|     ITC|  -0.62534785|\n",
      "|ALOKTEXT|    NHPC|  -0.33097458|\n",
      "|     ITC|ASHOKLEY|   -0.3144176|\n",
      "|    STER|     ITC|   -0.3065829|\n",
      "|GMRINFRA|    IDFC|  -0.28986531|\n",
      "|     ITC|  RENUKA|  -0.21256758|\n",
      "|     DLF|ALOKTEXT|  -0.16802602|\n",
      "|GMRINFRA|    NHPC| -0.048641354|\n",
      "|HINDALCO|    IDFC|-0.0068381117|\n",
      "+--------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the DataFrame df_corr_neg\n",
    "df_corr_neg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 921:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-JAN-2013 31-DEC-2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtering the DataFrame to select only EQUITY related data for the year 2013\n",
    "df_2013 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2013'\")\n",
    "\n",
    "# Extracting the first and last trading days of 2013\n",
    "first_day_2013 = (df_2013.select(\"TIMESTAMP\")\n",
    "                  .filter(\"TIMESTAMP like '%JAN-2013'\")\n",
    "                  .distinct()\n",
    "                  .orderBy(\"TIMESTAMP\")\n",
    "                  .first())[0]\n",
    "last_day_2013 = (df_2013.select(\"TIMESTAMP\")\n",
    "                 .filter(\"TIMESTAMP like '%DEC-2013'\")\n",
    "                 .distinct()\n",
    "                 .orderBy(\"TIMESTAMP\", ascending=False)\n",
    "                 .first())[0]\n",
    "\n",
    "# Printing the first and last trading days of 2013\n",
    "print(first_day_2013, last_day_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_on_day(loc_stock, loc_date):\n",
    "    \"\"\"\n",
    "    Function to get the closing price of a stock on a specific date.\n",
    "    \n",
    "    Args:\n",
    "        loc_stock (str): Symbol of the stock.\n",
    "        loc_date (str): Date in the format \"dd-MMM-yyyy\".\n",
    "    \n",
    "    Returns:\n",
    "        float: Closing price of the stock on the specified date.\n",
    "    \"\"\"\n",
    "    # Filtering the DataFrame to select the closing price of the specified stock on the specified date\n",
    "    loc_price = df_2013.where(F.col(\"TIMESTAMP\") == loc_date).where(F.col(\"SYMBOL\") == loc_stock).select(\"CLOSE\").collect()[0]\n",
    "    # Extracting the closing price from the collected result\n",
    "    return loc_price[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selected stocks, based on the analysis\n",
    "# |     ITC|ALOKTEXT|  -0.90314275|\n",
    "# |GMRINFRA|     ITC|   -0.7135044|\n",
    "# |    IDFC|ALOKTEXT|   -0.6409445|\n",
    "# |HINDALCO|     ITC|  -0.62534785|\n",
    "# |ALOKTEXT|    NHPC|  -0.33097458|\n",
    "\n",
    "stock_list = [\"ITC\",\"ALOKTEXT\",\"GMRINFRA\",\"IDFC\",\"HINDALCO\",\"NHPC\"]\n",
    "multiplier = [3,3,1,1,1,1]\n",
    "\n",
    "# Initializing lists to store prices and calculating total profit and investment\n",
    "prices = []\n",
    "total_profit = 0\n",
    "total_investment = 0\n",
    "\n",
    "# Looping through each stock and multiplier\n",
    "for the_stock, the_multiplier in zip(stock_list, multiplier):\n",
    "    # Getting the price of the stock on the first and last trading days of 2013\n",
    "    first_day_price = get_price_on_day(the_stock, first_day_2013)\n",
    "    last_day_price = get_price_on_day(the_stock, last_day_2013)\n",
    "    \n",
    "    # Calculating the difference in price and total difference\n",
    "    diff = (last_day_price - first_day_price)\n",
    "    total_diff = diff * the_multiplier\n",
    "    \n",
    "    # Adding to total profit and investment\n",
    "    total_profit += total_diff\n",
    "    total_investment += (first_day_price * the_multiplier)\n",
    "    \n",
    "    # Appending stock, first day price, last day price, price difference, and total difference to prices list\n",
    "    prices.append([the_stock, first_day_price, last_day_price, diff, total_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ITC', 287.25, 321.85, 34.60000000000002, 103.80000000000007],\n",
       " ['ALOKTEXT', 11.35, 8.45, -2.9000000000000004, -8.700000000000001],\n",
       " ['GMRINFRA', 20.3, 24.8, 4.5, 4.5],\n",
       " ['IDFC', 173.65, 109.6, -64.05000000000001, -64.05000000000001],\n",
       " ['HINDALCO', 134.15, 122.6, -11.550000000000011, -11.550000000000011],\n",
       " ['NHPC', 25.35, 19.55, -5.800000000000001, -5.800000000000001]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the prices list\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249.25 18.200000000000042\n"
     ]
    }
   ],
   "source": [
    "# Printing the total investment and total profit\n",
    "print(total_investment, total_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+------+---------+---------+---------------+-----------+------+\n",
      "|    SYMBOL|SERIES|  OPEN|  HIGH|   LOW| CLOSE|  LAST|PREVCLOSE|TOTTRDQTY|      TOTTRDVAL|  TIMESTAMP| ADDNL|\n",
      "+----------+------+------+------+------+------+------+---------+---------+---------------+-----------+------+\n",
      "|  ALOKTEXT|    EQ|  8.45|   9.0|  8.25|  8.85|  8.95|      8.2|  5003130|   4.31233725E7|01-APR-2013|  5614|\n",
      "|  ASHOKLEY|    EQ|  21.9| 22.25|  21.9| 22.15|  22.1|    21.85|  1420748|  3.134177775E7|01-APR-2013|  5424|\n",
      "|BHARTIARTL|    EQ| 288.0| 294.9| 288.0|293.65|293.45|   291.75|  1452020|  4.245766875E8|01-APR-2013| 19825|\n",
      "|      BHEL|    EQ|178.95| 184.4| 178.8| 182.5| 181.9|    177.0|  3082804|  5.617141141E8|01-APR-2013| 33593|\n",
      "|     CAIRN|    EQ| 274.4| 286.9| 274.2|286.05| 286.3|   272.45|  3067330| 8.6449004395E8|01-APR-2013| 55242|\n",
      "|    DISHTV|    EQ|  68.1| 69.15|  67.5|  68.0| 67.95|     67.1|  1833635|  1.249482505E8|01-APR-2013| 12735|\n",
      "|       DLF|    EQ| 238.0|254.35| 236.6| 253.2| 253.0|    234.7| 13591992| 3.3583141868E9|01-APR-2013|102085|\n",
      "|  GMRINFRA|    EQ|  21.6| 23.05|  21.5| 22.65|  22.5|     21.6|  8263825|  1.844829846E8|01-APR-2013| 16170|\n",
      "|      HDFC|    EQ| 826.8| 835.4|817.95| 825.2|827.55|   826.25|  1514565|1.25140996725E9|01-APR-2013| 54461|\n",
      "|  HINDALCO|    EQ|  92.2|  92.7|  90.5|  91.7|  91.5|     91.6|  4414052| 4.0476637785E8|01-APR-2013| 25876|\n",
      "+----------+------+------+------+------+------+------+---------+---------+---------------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 04:44:40 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 12\n",
      "CSV file: file:///home/hduser/spark/nsedata.csv\n"
     ]
    }
   ],
   "source": [
    "# Selecting the top 25 symbols with high average total traded quantity\n",
    "top25 = df_2012_avgqty.limit(25)\n",
    "\n",
    "# Collecting the symbols from the DataFrame top25\n",
    "t1 = top25.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filtering the DataFrame df_2013 to include only the symbols present in t1\n",
    "t2 = df_2013.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "\n",
    "# Showing the first 10 rows of the filtered DataFrame t2\n",
    "t2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating the total closing price on the first trading day of 2013 for all selected symbols\n",
    "first_day_overall = t2.where(F.col(\"TIMESTAMP\") == first_day_2013).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "\n",
    "# Calculating the total closing price on the last trading day of 2013 for all selected symbols\n",
    "last_day_overall = t2.where(F.col(\"TIMESTAMP\") == last_day_2013).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "\n",
    "# Calculating the total profit for all selected symbols during 2013\n",
    "total_profit_overall = last_day_overall[0][0] - first_day_overall[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount: 5119.3 invested on the first trading day of 2013\n",
      "has a value: 4226.45 on the last trading day of 2013\n",
      "The profit/loss is : -892.85 corresponding to -17.44%\n"
     ]
    }
   ],
   "source": [
    "# Printing the investment value on the first trading day of 2013, value on the last trading day of 2013,\n",
    "# and the corresponding profit/loss and percentage change\n",
    "print(f\"Amount: {first_day_overall[0][0]} invested on the first trading day of 2013\\n\\\n",
    "has a value: {last_day_overall[0][0]} on the last trading day of 2013\\n\\\n",
    "The profit/loss is : {total_profit_overall:.2f} corresponding to {total_profit_overall/first_day_overall[0][0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Performance of the strategy</b>\n",
    "- If we had invested in all the top 25 stocks, without implementing the negative correlation strategy, \n",
    "There would have been a loss of 892 on an investment of 5119 (17.5% loss)\n",
    "- As against that, by implementing the 'select based on negative correlation' strategy, \n",
    "a profit of 18.2 on an investment of 1249 (1.5% profit) has been achieved\n",
    "- In conclusion, the strategy has definitely prevented portfolio value loss during a bad year. It has, in fact, preserved capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Part 2 : <b>Problem to solve</b></b>\n",
    "1. Which of the following is better, if you have 10 Lakhs to invest for a year: \n",
    "    - identify 5 top performing stocks of the previous year and invest in them, or\n",
    "    - Spread your investment across a basket of 25 stocks, with investments equally distributed among them\n",
    "    - Employing strategies like 'negative correlation' to select your stocks\n",
    "    - What if you use 'positive correlation' instead, carry out analysis to understand the portfolio's performance?\n",
    "2. Do your analysis over multiple years (2011-1012, 2012-2013, etc.) to make your final recommendations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of Investment Strategies\n",
    "To determine the better investment strategy for an amount of 10 Lakhs invested for a year, we conducted the following analyses:\n",
    "\n",
    "### Strategy 1: Identify 5 Top Performing Stocks of the Previous Year\n",
    "Investing in the top 5 performing stocks from the previous year resulted in a profit percentage of approximately 8.89%.\n",
    "\n",
    "### Strategy 2: Spread Investment Across a Basket of 25 Stocks\n",
    "Investing equally across a random selection of 25 stock\n",
    "s yielded a significantly higher profit percentage of about 52.65%.\n",
    "\n",
    "### Strategy 3: Employing Negative Correlation\n",
    "Using a strategy based on negative correlation among selected stocks returned a profit percentage of around 30.20%.\n",
    "\n",
    "### Strategy 4: Employing Positive Correlation\n",
    "Utilizing a strategy based on positive correlation among selected stocks led to a profit percentage of approximately 46.59%.\n",
    "\n",
    "## 2. Analysis Over Multiple Years\n",
    "\n",
    "Overall, based on our analysis, investing in a diversified portfolio of randomly selected 25 stocks appeared to be the most profitable strategy, yielding the highest profit percentage among the four strategies considered. However, further analysis and evaluation may be necessary to account for various factors such as risk tolerance, market volatility, and investment goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|    SYMBOL|   avg(TOTTRDQTY)|\n",
      "+----------+-----------------+\n",
      "|  ALOKTEXT|8677144.275303643|\n",
      "|    GVKPIL|8149384.765182186|\n",
      "|  HINDALCO|7992351.906882592|\n",
      "|      RCOM|7713584.137651822|\n",
      "|    RENUKA|7459392.910931174|\n",
      "|SHREEASHTA|7339390.076923077|\n",
      "|       ITC|7325373.246963562|\n",
      "|      IDFC|7102852.137651822|\n",
      "|      HDIL|6585712.372469636|\n",
      "|TATAMOTORS|6425309.267206478|\n",
      "+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 04:49:59 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 12\n",
      "CSV file: file:///home/hduser/spark/nsedata.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+------+------+-----+------+---------+---------+---------------+-----------+-----+\n",
      "|  SYMBOL|SERIES| OPEN|  HIGH|   LOW|CLOSE|  LAST|PREVCLOSE|TOTTRDQTY|      TOTTRDVAL|  TIMESTAMP|ADDNL|\n",
      "+--------+------+-----+------+------+-----+------+---------+---------+---------------+-----------+-----+\n",
      "|ALOKTEXT|    EQ|15.75|  15.8|  15.4| 15.5|  15.5|    15.75|  8534303|  1.329065281E8|01-AUG-2012|22213|\n",
      "|  GVKPIL|    EQ| 13.0| 13.25|  12.9|13.15|  13.2|    12.95|  7620847| 1.0011849285E8|01-AUG-2012| 9278|\n",
      "|HINDALCO|    EQ|120.0| 120.8|118.35|119.2|118.85|    120.1|  3328980| 3.9755439225E8|01-AUG-2012|26926|\n",
      "|    RCOM|    EQ| 56.0| 57.25|  55.5|56.45| 56.35|     56.1|  9125929|  5.154176282E8|01-AUG-2012|32026|\n",
      "|  RENUKA|    EQ| 31.0| 32.15|  30.6| 31.9|  31.9|     30.9|  6345401| 1.9965185815E8|01-AUG-2012|16635|\n",
      "|ALOKTEXT|    EQ|20.25| 20.65| 19.95|20.55| 20.55|    20.05|  6174610| 1.2570505405E8|01-FEB-2012|14346|\n",
      "|  GVKPIL|    EQ| 16.0| 17.05|  15.4| 16.7| 16.95|     15.9| 33904855| 5.5234462945E8|01-FEB-2012|42477|\n",
      "|HINDALCO|    EQ|147.2| 153.8|144.35|153.2| 153.4|    146.6| 12920558| 1.9319594557E9|01-FEB-2012|92803|\n",
      "|    RCOM|    EQ|98.65|101.45|  98.1|100.4| 100.7|    99.25| 12026230|1.20246885395E9|01-FEB-2012|59624|\n",
      "|  RENUKA|    EQ| 38.0|  40.1|  37.4| 39.7|  39.8|    38.05| 13682362|   5.35343319E8|01-FEB-2012|41548|\n",
      "+--------+------+-----+------+------+-----+------+---------+---------+---------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Year = 2012\n",
    "#Top 5 stocks of previous year\n",
    "\n",
    "# Filtering the DataFrame to select only EQUITY related data for the year 2012\n",
    "df_2011 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2011'\")\n",
    "\n",
    "# Calculating the average total traded quantity for each symbol in the DataFrame df_2012\n",
    "# Filtering symbols with average total traded quantity between 5L and 10L\n",
    "# Sorting the DataFrame in descending order based on average total traded quantity\n",
    "df_2011_avgqty = df_2011.groupBy(\"SYMBOL\").avg(\"TOTTRDQTY\")\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") < 10000000)\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") > 500000)\\\n",
    "                        .orderBy(\"avg(TOTTRDQTY)\", ascending=False)\n",
    "# Showing the top 10 symbols with high average total traded quantity\n",
    "df_2011_avgqty.show(10)\n",
    "\n",
    "# Selecting the top 5 symbols with high average total traded quantity\n",
    "top5 = df_2011_avgqty.limit(5)\n",
    "\n",
    "# Collecting the symbols from the DataFrame top5\n",
    "t1 = top5.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filtering the DataFrame df_2012 to include only the symbols present in t1\n",
    "t2 = df_2012.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "\n",
    "# Showing the first 10 rows of the filtered DataFrame t2\n",
    "t2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-JAN-2012 31-DEC-2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtering the DataFrame to select only EQUITY related data for the year 2013\n",
    "df_2012 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2012'\")\n",
    "\n",
    "# Extracting the first and last trading days of 2013\n",
    "first_day_2012 = (df_2012.select(\"TIMESTAMP\")\n",
    "                  .filter(\"TIMESTAMP like '%JAN-2012'\")\n",
    "                  .distinct()\n",
    "                  .orderBy(\"TIMESTAMP\")\n",
    "                  .first())[0]\n",
    "last_day_2012 = (df_2012.select(\"TIMESTAMP\")\n",
    "                 .filter(\"TIMESTAMP like '%DEC-2012'\")\n",
    "                 .distinct()\n",
    "                 .orderBy(\"TIMESTAMP\", ascending=False)\n",
    "                 .first())[0]\n",
    "\n",
    "# Printing the first and last trading days of 2013\n",
    "print(first_day_2012, last_day_2012)\n",
    "\n",
    "# Calculating the total closing price on the first trading day of 2012 for all selected symbols\n",
    "first_day_overall = t2.where(F.col(\"TIMESTAMP\") == first_day_2012).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "\n",
    "# Calculating the total closing price on the last trading day of 2013 for all selected symbols\n",
    "last_day_overall = t2.where(F.col(\"TIMESTAMP\") == last_day_2012).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "\n",
    "# Calculating the total profit for all selected symbols during 2012\n",
    "total_profit_overall = last_day_overall[0][0] - first_day_overall[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount: 239.5 invested on the first trading day of 2012\n",
      "has a value: 260.8 on the last trading day of 2012\n",
      "The profit/loss is : 21.30 corresponding to 8.89%\n"
     ]
    }
   ],
   "source": [
    "# Printing the investment value on the first trading day of 2012, value on the last trading day of 2012,\n",
    "# and the corresponding profit/loss and percentage change\n",
    "print(f\"Amount: {first_day_overall[0][0]} invested on the first trading day of 2012\\n\\\n",
    "has a value: {last_day_overall[0][0]} on the last trading day of 2012\\n\\\n",
    "The profit/loss is : {total_profit_overall:.2f} corresponding to {total_profit_overall/first_day_overall[0][0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spreading across 25 random stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|    SYMBOL|   avg(TOTTRDQTY)|\n",
      "+----------+-----------------+\n",
      "|  ALOKTEXT|8677144.275303643|\n",
      "|    GVKPIL|8149384.765182186|\n",
      "|  HINDALCO|7992351.906882592|\n",
      "|      RCOM|7713584.137651822|\n",
      "|    RENUKA|7459392.910931174|\n",
      "|SHREEASHTA|7339390.076923077|\n",
      "|       ITC|7325373.246963562|\n",
      "|      IDFC|7102852.137651822|\n",
      "|      HDIL|6585712.372469636|\n",
      "|TATAMOTORS|6425309.267206478|\n",
      "+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Year = 2012\n",
    "#Top 5 stocks of previous year\n",
    "\n",
    "# Filtering the DataFrame to select only EQUITY related data for the year 2012\n",
    "df_2011 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2011'\")\n",
    "\n",
    "# Calculating the average total traded quantity for each symbol in the DataFrame df_2012\n",
    "# Filtering symbols with average total traded quantity between 5L and 10L\n",
    "# Sorting the DataFrame in descending order based on average total traded quantity\n",
    "df_2011_avgqty = df_2011.groupBy(\"SYMBOL\").avg(\"TOTTRDQTY\")\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") < 10000000)\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") > 500000)\\\n",
    "                        .orderBy(\"avg(TOTTRDQTY)\", ascending=False)\n",
    "# Showing the top 10 symbols with high average total traded quantity\n",
    "df_2011_avgqty.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ESSAROIL', 'SKUMARSYNF', 'CIPLA', 'HINDPETRO', 'ASHOKLEY', 'KSOILS', 'ONELIFECAP', 'HDIL', 'KFA', 'RELCAPITAL', 'NUTEK', 'TAKSHEEL', 'INDOTHAI', 'KOTAKBANK', 'RESURGERE', 'BALLARPUR', 'KWALITY', 'RANBAXY', 'AMTEKINDIA', 'SUNTV', 'BAJAJHIND', 'ZEELEARN', 'RUCHISOYA', 'APOLLOTYRE', 'IBWSL']\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary module\n",
    "import random\n",
    "\n",
    "# Selecting 25 random stocks from df_2011_avgqty\n",
    "random_stocks = random.sample(df_2011_avgqty.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect(), 25)\n",
    "\n",
    "# Displaying the randomly selected stocks\n",
    "print(random_stocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+-----+------+-----+-----+---------+---------+---------------+-----------+-----+\n",
      "|    SYMBOL|SERIES| OPEN| HIGH|   LOW|CLOSE| LAST|PREVCLOSE|TOTTRDQTY|      TOTTRDVAL|  TIMESTAMP|ADDNL|\n",
      "+----------+------+-----+-----+------+-----+-----+---------+---------+---------------+-----------+-----+\n",
      "|AMTEKINDIA|    EQ| 90.6| 92.5|  90.6| 91.6| 91.9|    91.35|   237030|  2.171735195E7|01-AUG-2012| 1517|\n",
      "|APOLLOTYRE|    EQ| 77.4|78.75| 76.55| 78.3| 78.5|    77.75|  2401348| 1.8671335015E8|01-AUG-2012|19979|\n",
      "|  ASHOKLEY|    EQ|21.85| 22.3|  21.8|22.05|22.05|     22.3|  5920768| 1.3014597305E8|01-AUG-2012|14778|\n",
      "| BAJAJHIND|    EQ|30.55|32.15| 30.25| 31.9| 31.8|     30.4|  2634337|    8.2736978E7|01-AUG-2012| 8563|\n",
      "| BALLARPUR|    EQ|19.05|19.55|  18.6|18.75|18.85|     19.2|   188511|      3571311.3|01-AUG-2012|  973|\n",
      "|     CIPLA|    EQ|360.0|363.6| 350.1|354.0|353.7|    338.6|  6571518| 2.3408779386E9|01-AUG-2012|77205|\n",
      "|  ESSAROIL|    EQ| 55.0|56.55| 54.15|56.05| 56.0|     55.6|  3230874| 1.7933284015E8|01-AUG-2012|19721|\n",
      "|      HDIL|    EQ| 78.5| 83.2|  78.5|82.65| 82.2|    79.35| 17264112|1.41062388995E9|01-AUG-2012|77072|\n",
      "| HINDPETRO|    EQ|345.0|345.0|336.65|337.9|337.5|    343.7|   586320| 1.9837667405E8|01-AUG-2012| 4768|\n",
      "|     IBWSL|    EQ| 10.0| 10.9|  10.0|10.35| 10.2|     10.2|   131479|     1360069.85|01-AUG-2012|  255|\n",
      "+----------+------+-----+-----+------+-----+-----+---------+---------+---------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 04:57:50 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 12\n",
      "CSV file: file:///home/hduser/spark/nsedata.csv\n"
     ]
    }
   ],
   "source": [
    "# Filtering the DataFrame df_2012 to include only the symbols present in t1\n",
    "t2 = df_2012.filter(F.col(\"SYMBOL\").isin(random_stocks))\n",
    "\n",
    "# Showing the first 10 rows of the filtered DataFrame t2\n",
    "t2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating the total closing price on the first trading day of 2012 for all selected symbols\n",
    "first_day_overall = t2.where(F.col(\"TIMESTAMP\") == first_day_2012).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "\n",
    "# Calculating the total closing price on the last trading day of 2013 for all selected symbols\n",
    "last_day_overall = t2.where(F.col(\"TIMESTAMP\") == last_day_2012).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "\n",
    "# Calculating the total profit for all selected symbols during 2012\n",
    "total_profit_overall = last_day_overall[0][0] - first_day_overall[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount: 2705.1 invested on the first trading day of 2012\n",
      "has a value: 4129.2 on the last trading day of 2012\n",
      "The profit/loss is : 1424.10 corresponding to 52.65%\n"
     ]
    }
   ],
   "source": [
    "# Printing the investment value on the first trading day of 2012, value on the last trading day of 2012,\n",
    "# and the corresponding profit/loss and percentage change\n",
    "print(f\"Amount: {first_day_overall[0][0]} invested on the first trading day of 2012\\n\\\n",
    "has a value: {last_day_overall[0][0]} on the last trading day of 2012\\n\\\n",
    "The profit/loss is : {total_profit_overall:.2f} corresponding to {total_profit_overall/first_day_overall[0][0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employing Negative correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top 10 symbols with high average total traded quantity\n",
    "top10 = df_2011_avgqty.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting the symbols from df_2011_avgqty DataFrame\n",
    "t1 = top10.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filtering the DataFrame df_2011 to include only the selected symbols\n",
    "t2 = df_2011.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "\n",
    "# Creating DataFrame t3 with columns SYMBOL as S1, CLOSE as Close1, and TIMESTAMP\n",
    "t3 = t2.select(F.col(\"SYMBOL\").alias(\"S1\"), F.col(\"CLOSE\").alias(\"Close1\"), \"TIMESTAMP\")\n",
    "\n",
    "# Creating DataFrame t4 with columns SYMBOL as S2, CLOSE as Close2, and TIMESTAMP\n",
    "t4 = t2.select(F.col(\"SYMBOL\").alias(\"S2\"), F.col(\"CLOSE\").alias(\"Close2\"), \"TIMESTAMP\")\n",
    "\n",
    "# Joining DataFrames t3 and t4 on TIMESTAMP to create DataFrame df_for_corr\n",
    "df_for_corr = t3.join(t4, \"TIMESTAMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----------+------+\n",
      "|  TIMESTAMP|      S1|Close1|        S2|Close2|\n",
      "+-----------+--------+------+----------+------+\n",
      "|12-JUL-2011|ALOKTEXT| 24.45|TATAMOTORS|1024.9|\n",
      "|12-JUL-2011|ALOKTEXT| 24.45|SHREEASHTA|   4.8|\n",
      "|12-JUL-2011|ALOKTEXT| 24.45|    RENUKA| 70.65|\n",
      "|12-JUL-2011|ALOKTEXT| 24.45|      RCOM|  96.8|\n",
      "|12-JUL-2011|ALOKTEXT| 24.45|       ITC|201.65|\n",
      "+-----------+--------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Showing the first 5 rows of the DataFrame df_for_corr\n",
    "df_for_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(S1='ITC', S2='SHREEASHTA'),\n",
       " Row(S1='GVKPIL', S2='IDFC'),\n",
       " Row(S1='RENUKA', S2='IDFC'),\n",
       " Row(S1='RCOM', S2='GVKPIL'),\n",
       " Row(S1='TATAMOTORS', S2='ITC'),\n",
       " Row(S1='HINDALCO', S2='IDFC'),\n",
       " Row(S1='ALOKTEXT', S2='HDIL'),\n",
       " Row(S1='IDFC', S2='TATAMOTORS'),\n",
       " Row(S1='ITC', S2='ALOKTEXT'),\n",
       " Row(S1='HDIL', S2='ALOKTEXT')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting distinct pairs of symbols where S1 is not equal to S2\n",
    "# Collecting the first 10 pairs as a list\n",
    "wrklist = df_for_corr.select(\"S1\",\"S2\").filter(\"S1 != S2\").distinct().collect()\n",
    "wrklist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Printing the length of the wrklist, which represents the number of pairs in the list\n",
    "print(len(wrklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 20 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 40 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 60 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 80 of 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initializing an empty list tcorr to store correlation values for stock pairs\n",
    "tcorr = []\n",
    "# Getting the length of wrklist\n",
    "tlen = len(wrklist)\n",
    "# Iterating through each pair of stocks in wrklist\n",
    "for i in range(tlen):\n",
    "    # Extracting the first and second stock symbols from the pair\n",
    "    s1 = wrklist[i][0]\n",
    "    s2 = wrklist[i][1]\n",
    "    # Calculating the correlation between the closing prices of the two stocks\n",
    "    corr = df_for_corr.filter((F.col('S1') == s1) & (F.col('S2') == s2)).corr(\"Close1\", \"Close2\")\n",
    "    # Appending the stock symbols and correlation value to tcorr list\n",
    "    tcorr.append([s1, s2, corr])\n",
    "    # Printing progress message for every 20 iterations\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processed: {i} of {tlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|   Symbol1|   Symbol2|       Corr|\n",
      "+----------+----------+-----------+\n",
      "|       ITC|SHREEASHTA|  -0.518378|\n",
      "|    GVKPIL|      IDFC| 0.87741727|\n",
      "|    RENUKA|      IDFC| 0.82532847|\n",
      "|      RCOM|    GVKPIL|  0.9186302|\n",
      "|TATAMOTORS|       ITC|-0.62484014|\n",
      "+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules and classes\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Defining the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Symbol1\", StringType(), True),\n",
    "    StructField(\"Symbol2\", StringType(), True),\n",
    "    StructField(\"Corr\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Creating an RDD from the list of correlations tcorr\n",
    "rdd = sc.parallelize(tcorr)\n",
    "\n",
    "# Creating a DataFrame df_corr from the RDD\n",
    "df_corr = ss.createDataFrame(rdd.map(lambda x: Row(Symbol1=x[0], Symbol2=x[1], Corr=float(x[2]))), schema)\n",
    "\n",
    "# Showing the first 5 rows of the DataFrame df_corr\n",
    "df_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the DataFrame df_corr to include only negative correlations and removing duplicates based on \"Corr\" column\n",
    "# Sorting the DataFrame in ascending order based on \"Corr\" column\n",
    "df_corr_neg = df_corr.filter(F.col(\"Corr\") <= 0.0).dropDuplicates([\"Corr\"]).orderBy(F.col(\"Corr\").asc())\n",
    "\n",
    "# Counting the number of rows in the DataFrame df_corr_neg\n",
    "df_corr_neg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|   Symbol1|   Symbol2|       Corr|\n",
      "+----------+----------+-----------+\n",
      "|  HINDALCO|       ITC| -0.7588449|\n",
      "|    GVKPIL|       ITC| -0.7576902|\n",
      "|       ITC|    RENUKA|  -0.627603|\n",
      "|TATAMOTORS|       ITC|-0.62484014|\n",
      "|       ITC|      RCOM| -0.5900066|\n",
      "|       ITC|      IDFC| -0.5551699|\n",
      "|       ITC|SHREEASHTA|  -0.518378|\n",
      "|       ITC|      HDIL|-0.48173952|\n",
      "|       ITC|  ALOKTEXT|-0.32108563|\n",
      "+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the DataFrame df_corr_neg\n",
    "df_corr_neg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_on_day(loc_stock, loc_date):\n",
    "    \"\"\"\n",
    "    Function to get the closing price of a stock on a specific date.\n",
    "    \n",
    "    Args:\n",
    "        loc_stock (str): Symbol of the stock.\n",
    "        loc_date (str): Date in the format \"dd-MMM-yyyy\".\n",
    "    \n",
    "    Returns:\n",
    "        float: Closing price of the stock on the specified date.\n",
    "    \"\"\"\n",
    "    # Filtering the DataFrame to select the closing price of the specified stock on the specified date\n",
    "    loc_price = df_2012.where(F.col(\"TIMESTAMP\") == loc_date).where(F.col(\"SYMBOL\") == loc_stock).select(\"CLOSE\").collect()[0]\n",
    "    # Extracting the closing price from the collected result\n",
    "    return loc_price[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selected stocks, based on the analysis\n",
    "# |  HINDALCO|       ITC| -0.7588449|\n",
    "# |    GVKPIL|       ITC| -0.7576902|\n",
    "# |       ITC|    RENUKA|  -0.627603|\n",
    "# |TATAMOTORS|       ITC|-0.62484014|\n",
    "# |       ITC|      RCOM| -0.5900066|\n",
    "\n",
    "stock_list = [\"HINDALCO\",\"ITC\",\"GVKPIL\",\"RENUKA\",\"TATAMOTORS\",\"RCOM\"]\n",
    "multiplier = [5,1,1,1,1,1]\n",
    "\n",
    "# Initializing lists to store prices and calculating total profit and investment\n",
    "prices = []\n",
    "total_profit = 0\n",
    "total_investment = 0\n",
    "\n",
    "# Looping through each stock and multiplier\n",
    "for the_stock, the_multiplier in zip(stock_list, multiplier):\n",
    "    # Getting the price of the stock on the first and last trading days of 2012\n",
    "    first_day_price = get_price_on_day(the_stock, first_day_2012)\n",
    "    last_day_price = get_price_on_day(the_stock, last_day_2012)\n",
    "    \n",
    "    # Calculating the difference in price and total difference\n",
    "    diff = (last_day_price - first_day_price)\n",
    "    total_diff = diff * the_multiplier\n",
    "    \n",
    "    # Adding to total profit and investment\n",
    "    total_profit += total_diff\n",
    "    total_investment += (first_day_price * the_multiplier)\n",
    "    \n",
    "    # Appending stock, first day price, last day price, price difference, and total difference to prices list\n",
    "    prices.append([the_stock, first_day_price, last_day_price, diff, total_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['HINDALCO', 112.25, 130.5, 18.25, 91.25],\n",
       " ['ITC', 198.65, 286.8, 88.15, 88.15],\n",
       " ['GVKPIL', 12.4, 13.55, 1.1500000000000004, 1.1500000000000004],\n",
       " ['RENUKA', 24.75, 31.75, 7.0, 7.0],\n",
       " ['TATAMOTORS', 183.95, 312.65, 128.7, 128.7],\n",
       " ['RCOM', 72.1, 73.9, 1.8000000000000114, 1.8000000000000114]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the prices list\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053.1 318.05\n"
     ]
    }
   ],
   "source": [
    "# Printing the total investment and total profit\n",
    "print(total_investment, total_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the DataFrame df_corr to include only positive correlations and removing duplicates based on \"Corr\" column\n",
    "# Sorting the DataFrame in ascending order based on \"Corr\" column\n",
    "df_corr_pos = df_corr.filter(F.col(\"Corr\") >= 0.0).dropDuplicates([\"Corr\"]).orderBy(F.col(\"Corr\").desc())\n",
    "\n",
    "# Counting the number of rows in the DataFrame df_corr_neg\n",
    "df_corr_pos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|   Symbol1|   Symbol2|      Corr|\n",
      "+----------+----------+----------+\n",
      "|    GVKPIL|  HINDALCO| 0.9386468|\n",
      "|      RCOM|    GVKPIL| 0.9186302|\n",
      "|  HINDALCO|TATAMOTORS|0.91859823|\n",
      "|      HDIL|TATAMOTORS| 0.9049866|\n",
      "|    RENUKA|    GVKPIL|  0.888308|\n",
      "|  HINDALCO|      IDFC| 0.8815824|\n",
      "|    GVKPIL|      IDFC|0.87741727|\n",
      "|    RENUKA|  HINDALCO|0.87294793|\n",
      "|  HINDALCO|      RCOM| 0.8715104|\n",
      "|      RCOM|      IDFC| 0.8642946|\n",
      "|  HINDALCO|      HDIL| 0.8599854|\n",
      "|      HDIL|      IDFC| 0.8481832|\n",
      "|      RCOM|    RENUKA| 0.8473944|\n",
      "|    RENUKA|      HDIL|0.82943594|\n",
      "|    RENUKA|      IDFC|0.82532847|\n",
      "|  ALOKTEXT|      HDIL| 0.8222778|\n",
      "|TATAMOTORS|    GVKPIL| 0.8216821|\n",
      "|    GVKPIL|      HDIL| 0.7959564|\n",
      "|    RENUKA|TATAMOTORS|0.79437506|\n",
      "|      IDFC|TATAMOTORS|0.79226977|\n",
      "+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the DataFrame df_corr_neg\n",
    "df_corr_pos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selected stocks, based on the analysis\n",
    "# |    GVKPIL|  HINDALCO| 0.9386468|\n",
    "# |      RCOM|    GVKPIL| 0.9186302|\n",
    "# |  HINDALCO|TATAMOTORS|0.91859823|\n",
    "# |      HDIL|TATAMOTORS| 0.9049866|\n",
    "# |    RENUKA|    GVKPIL|  0.888308|\n",
    "\n",
    "stock_list = [\"GVKPIL\",\"HINDALCO\",\"RCOM\",\"TATAMOTORS\",\"HDIL\",\"RENUKA\"]\n",
    "multiplier = [3,2,1,2,1,1]\n",
    "\n",
    "# Initializing lists to store prices and calculating total profit and investment\n",
    "prices = []\n",
    "total_profit = 0\n",
    "total_investment = 0\n",
    "\n",
    "# Looping through each stock and multiplier\n",
    "for the_stock, the_multiplier in zip(stock_list, multiplier):\n",
    "    # Getting the price of the stock on the first and last trading days of 2012\n",
    "    first_day_price = get_price_on_day(the_stock, first_day_2012)\n",
    "    last_day_price = get_price_on_day(the_stock, last_day_2012)\n",
    "    \n",
    "    # Calculating the difference in price and total difference\n",
    "    diff = (last_day_price - first_day_price)\n",
    "    total_diff = diff * the_multiplier\n",
    "    \n",
    "    # Adding to total profit and investment\n",
    "    total_profit += total_diff\n",
    "    total_investment += (first_day_price * the_multiplier)\n",
    "    \n",
    "    # Appending stock, first day price, last day price, price difference, and total difference to prices list\n",
    "    prices.append([the_stock, first_day_price, last_day_price, diff, total_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GVKPIL', 12.4, 13.55, 1.1500000000000004, 3.450000000000001],\n",
       " ['HINDALCO', 112.25, 130.5, 18.25, 36.5],\n",
       " ['RCOM', 72.1, 73.9, 1.8000000000000114, 1.8000000000000114],\n",
       " ['TATAMOTORS', 183.95, 312.65, 128.7, 257.4],\n",
       " ['HDIL', 54.05, 111.5, 57.45, 57.45],\n",
       " ['RENUKA', 24.75, 31.75, 7.0, 7.0]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the prices list\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780.4999999999999 363.59999999999997\n"
     ]
    }
   ],
   "source": [
    "# Printing the total investment and total profit\n",
    "print(total_investment, total_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
